{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 19:04:24.801859 12060 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0830 19:04:25.336315 12060 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0830 19:04:25.435526 12060 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0830 19:04:25.536000 12060 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0830 19:04:25.557912 12060 deprecation.py:506] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0830 19:04:25.827095 12060 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "class Game:\n",
    "    def __init__(self,board =[[5,5,5,5,5],[5,5,5,5,5]],turn = 1, score1 = 0,score2 = 0):\n",
    "        self.finish = False\n",
    "        r=0\n",
    "        self.board= [[5,5,5,5,5], [5,5,5,5,5]]\n",
    "        for i in range(2):\n",
    "            for j in range(5):\n",
    "                self.board[i][j]=board[i][j]\n",
    "        self.score1 = score1\n",
    "        self.score2 = score2\n",
    "        self.turn = turn\n",
    "    def put(self,board,turn,score1,score2):\n",
    "        self.board= [[5,5,5,5,5], [5,5,5,5,5]]\n",
    "        for i in range(2):\n",
    "            for j in range(5):\n",
    "                self.board[i][j]=board[i][j]\n",
    "        self.score1 = score1\n",
    "        self.score2 = score2\n",
    "        self.turn = turn\n",
    "    def do_move(self,move):\n",
    "        i = 0\n",
    "        last = 0\n",
    "        MOVE = 0\n",
    "        if(move == 6):\n",
    "            MOVE = 10 + self.turn\n",
    "            move = 3\n",
    "        balls = self.board[self.turn-1][move-1]\n",
    "        if(balls <= 1):\n",
    "            return -1\n",
    "        if(MOVE <= 10):\n",
    "            MOVE = move - 1 + 5 *(self.turn - 1)\n",
    "        self.board[self.turn -1][move-1] = 0\n",
    "        if(MOVE in [0,1,2,9,8,7]):\n",
    "            if MOVE == 0: i=0\n",
    "            elif MOVE == 9: i=5\n",
    "            elif MOVE in [1,2]: i=10-MOVE\n",
    "            else: i=MOVE-4\n",
    "            for b in range(balls):\n",
    "                k=i%10\n",
    "                if(k<5):\n",
    "                    self.board[1][k] += 1\n",
    "                    if b==balls-1: last = self.board[1][k]\n",
    "                    if(last in [2,4] and self.turn==1):\n",
    "                        self.board[1][k]=0\n",
    "                        self.score1 += last\n",
    "                        for l in range(1,k+1):\n",
    "                            last = self.board[1][k-l]\n",
    "                            if(last in [2,4]):\n",
    "                                self.score1 += last\n",
    "                                self.board[1][k-l]=0\n",
    "                            else: return 1\n",
    "                        return 1\n",
    "                else:\n",
    "                    self.board[0][9-k] += 1\n",
    "                    if b==balls-1: last = self.board[0][9-k]\n",
    "                    if(last in [2,4] and self.turn==2):\n",
    "                        self.board[0][9-k]=0\n",
    "                        self.score2 += last\n",
    "                        for l in range(1,k-4):\n",
    "                            last = self.board[0][9-k+l]\n",
    "                            if(last in [2,4]):\n",
    "                                self.score2 += last\n",
    "                                self.board[0][9-k+l]=0\n",
    "                            else: return 1\n",
    "                        return 1\n",
    "                i+=1\n",
    "        else:\n",
    "            if MOVE == 4: i = 0\n",
    "            elif MOVE == 5: i = 5\n",
    "            elif MOVE == 6: i = 4\n",
    "            elif MOVE == 3: i = 9\n",
    "            elif MOVE == 11:i = 8\n",
    "            else: i = 3\n",
    "            for b in range(balls):\n",
    "                k=i%10\n",
    "                if(k<5):\n",
    "                    self.board[1][4-k] += 1\n",
    "                    if b==balls-1: last = self.board[1][4-k]\n",
    "                    if(last in [2,4] and self.turn==1):\n",
    "                        self.board[1][4-k]=0\n",
    "                        self.score1 += last\n",
    "                        for l in range(1,k+1):\n",
    "                            last = self.board[1][4-k+l]\n",
    "                            if(last in [2,4]):\n",
    "                                self.score1 += last\n",
    "                                self.board[1][4-k+l]=0\n",
    "                            else: return 1\n",
    "                        return 1\n",
    "                else:\n",
    "                    self.board[0][k-5] += 1\n",
    "                    if b==balls-1: last = self.board[0][k-5]\n",
    "                    if(last in [2,4] and self.turn==2):\n",
    "                        self.board[0][k-5]=0\n",
    "                        self.score2 += last\n",
    "                        for l in range(1,k-4):\n",
    "                            last = self.board[0][k-5-l]\n",
    "                            if(last in [2,4]):\n",
    "                                self.score2 += last\n",
    "                                self.board[0][k-5-l]=0\n",
    "                            else: return 1\n",
    "                        return 1\n",
    "                i+=1\n",
    "        return 0    \n",
    "    def check_turn(self):\n",
    "        for j in range(5):\n",
    "            if(self.board[self.turn-1][j] >= 2):\n",
    "                return False\n",
    "        return True\n",
    "    def check_finish(self):\n",
    "        for i in range(2):\n",
    "            for j in range(5):\n",
    "                if(self.board[i][j] >= 2):\n",
    "                    return False\n",
    "        self.score1 += sum(self.board[0])\n",
    "        self.score2 += sum(self.board[1])\n",
    "        self.finish = True\n",
    "        return True\n",
    "    def print_game(self,sc =True):\n",
    "        for i in range(2):\n",
    "            print(self.board[i][0],self.board[i][1],self.board[i][2],self.board[i][3],self.board[i][4])\n",
    "        print(\"[ \",self.score1,\" , \",self.score2,\" ]\")\n",
    "    def flip_turn(self):\n",
    "        if self.turn==1: self.turn=2\n",
    "        else: self.turn=1\n",
    "            \n",
    "class Player:\n",
    "    def __init__(self,PType=\"human\",agent = None,depth=3):\n",
    "        self.type = PType\n",
    "        self.depth = depth\n",
    "        if(PType==\"DQN\"):\n",
    "            if(agent != None): self.agent = agent\n",
    "            else: self.agent = DQNAgent()\n",
    "    def get_move(self,game):\n",
    "        if(self.type == \"human\"):\n",
    "            return int(input(\"Enter move:\"))\n",
    "        elif(self.type == \"random\"):\n",
    "            return np.random.randint(1,6)\n",
    "        elif(self.type==\"DQN\"):\n",
    "            return np.argmax(self.agent.model.predict(np.array(game.board).reshape((1,10)))[0])\n",
    "        elif(self.type==\"minmax\"):\n",
    "            return min_max(game.board,self.depth,game.turn,game.score1,game.score2)\n",
    "        else:\n",
    "            return 0\n",
    "def min_max(board,depth,turn,s1,s2):\n",
    "    G = Game(board=board,turn=turn,score1=s1,score2=s2)\n",
    "    Max = -100\n",
    "    Best = np.random.randint(1,6)\n",
    "    for i in range(1,7):\n",
    "        G.put(board,turn,s1,s2)\n",
    "        if(G.do_move(i)==-1):\n",
    "            continue\n",
    "        d = r_min_max(G.board,G.turn,G.score1,G.score2,depth,turn)\n",
    "        if(d > Max):\n",
    "            Max = d\n",
    "            Best = i\n",
    "    del(G)\n",
    "    return Best\n",
    "\n",
    "def r_min_max(board,turn,s1,s2,depth,orig):\n",
    "    depth -= 1\n",
    "    if depth==0: \n",
    "        if orig==2: return s2-s1\n",
    "        return s1-s2\n",
    "    G= Game(board,turn,s1,s2)\n",
    "    G.flip_turn()\n",
    "    turn=G.turn\n",
    "    is_max = False\n",
    "    MinMax = 100\n",
    "    if G.turn==orig: \n",
    "        is_max=True\n",
    "        MinMax = -100\n",
    "    if(G.check_finish()):\n",
    "        if orig==1: return G.score1-G.score2\n",
    "        return G.score2-G.score1\n",
    "    elif(G.check_turn()):\n",
    "        if orig==1: return G.score1-G.score2\n",
    "        return G.score2-G.score1\n",
    "    for i in range(1,7):\n",
    "        G.put(board,turn,s1,s2)\n",
    "        if G.do_move(i)==-1:\n",
    "            delt = G.score1-G.score2\n",
    "            if orig==1: return delt\n",
    "            return -1*delt\n",
    "        d = r_min_max(G.board,G.turn,G.score1,G.score2,depth,orig)\n",
    "        if(is_max and d>MinMax):\n",
    "            MinMax = d\n",
    "        elif(not is_max and d<MinMax):\n",
    "            MinMax = d\n",
    "    del(G)\n",
    "    return MinMax\n",
    "            \n",
    "        \n",
    "        \n",
    "    return 0\n",
    "def plot_seaborn(array_counter, array_score):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='score')\n",
    "    plt.show()\n",
    "def timer(st=False,p=0):\n",
    "    if st:\n",
    "        return time.time()\n",
    "    else:\n",
    "        print(\"ok\")\n",
    "        print(\"Total Time = %f\" % (p-time.time()))\n",
    "        return p-time.time()\n",
    "def run(player1,player2,numGames):\n",
    "    win1 = 0\n",
    "    win2 = 0\n",
    "    tie = 0\n",
    "    for gm in range(numGames):\n",
    "        print(gm+1)\n",
    "        g=Game()\n",
    "        while(not g.check_finish()):\n",
    "            r=-1\n",
    "            while(r==-1 and not g.check_turn()):\n",
    "                if g.turn==1: m = player1.get_move(g)\n",
    "                else: m = player2.get_move(g)\n",
    "                r = g.do_move(m)\n",
    "                if(r==-1): g.do_move(np.random.randint(1,6))\n",
    "            #print(g.turn,m)\n",
    "            #g.print_game()\n",
    "            g.flip_turn()\n",
    "            if(player1.type==\"human\" or player2.type==\"human\"): print(m);g.print_game()\n",
    "        #g.print_game()\n",
    "        if(g.score1>g.score2): win1 += 1\n",
    "        elif(g.score1<g.score2): win2 += 1\n",
    "        else: tie += 1\n",
    "        del(g)\n",
    "    print(\"Win 1 =\",win1,\"Win 2 =\",win2,\"Tie =\",tie)\n",
    "    return 0\n",
    "\n",
    "def train(agent = DQNAgent(),trainer = Player(\"random\"),numGames = 100):\n",
    "    print(\"Training on\",numGames,\" games.\")\n",
    "    score_plot = []\n",
    "    counter_plot = []\n",
    "    for gm in range(numGames):\n",
    "        #print(\"Playing game #\",gm+1)\n",
    "        g=Game(turn = 2)\n",
    "        agent.epsilon = numGames/2 - gm\n",
    "        while(not g.check_finish()):\n",
    "            r = -1\n",
    "            old_board = np.array(g.board)\n",
    "            old_scores = [g.score1,g.score2]\n",
    "            g.turn = 2\n",
    "            \n",
    "            #print(\"Agent playing\")\n",
    "            c1 = not g.finish\n",
    "            while(r==-1 and not g.check_turn()):\n",
    "                if random.randint(0, int(numGames/2)) < agent.epsilon:\n",
    "                    m = random.randint(1,6)\n",
    "                    final_move = keras.utils.to_categorical(m-1, num_classes=6)\n",
    "                    r = g.do_move(m)\n",
    "                    #print(\"random =\",m)\n",
    "                else:\n",
    "                    m = np.argmax(agent.model.predict(np.array(g.board).reshape((1,10)))[0])+1\n",
    "                    #print(\"prediction =\",m)\n",
    "                    final_move = keras.utils.to_categorical(m-1, num_classes=6)\n",
    "                    r = g.do_move(m)\n",
    "                    if(r==-1):\n",
    "                        while(r==-1 and not g.check_turn()):\n",
    "                            m = random.randint(1,6)\n",
    "                            final_move = keras.utils.to_categorical(m-1, num_classes=6)\n",
    "                            r = g.do_move(m)\n",
    "                c1 = g.check_turn()\n",
    "            r=-1\n",
    "            c2= not g.finish\n",
    "            g.turn = 1\n",
    "            #print(m)\n",
    "            #g.print_game()\n",
    "            #print(\"Trainer playing\")\n",
    "            while(r==-1 and not g.check_turn()):\n",
    "                m = trainer.get_move(g)\n",
    "                #print(m,g.turn)\n",
    "                r = g.do_move(m)\n",
    "                if r ==-1: g.do_move(random.randint(1,6))\n",
    "                c2 = g.check_turn()\n",
    "            #print(m)\n",
    "            #g.print_game()\n",
    "            #print(\"final:\",m)\n",
    "            if(not c1):\n",
    "                #print(\"saving a move\")\n",
    "                Estart=time.time()\n",
    "                new_board = np.array(g.board)\n",
    "                new_scores = [g.score1,g.score2]\n",
    "                reward = agent.set_reward(old_scores[1]-old_scores[0],new_scores[1]-new_scores[0])\n",
    "                agent.train_short_memory(old_board, final_move, reward, new_board, g.finish)\n",
    "                agent.remember(old_board.reshape((10)), final_move, reward, new_board.reshape((10)),g.finish)\n",
    "                #print(\"move saved with parameters:\",old_board, final_move, reward, new_board, g.finish)\n",
    "                #start=time.time()-start\n",
    "                #eprint(\"getting a state time = %f\" % (start))\n",
    "        #start= time.time()\n",
    "        agent.replay_new(agent.memory)\n",
    "        #start=time.time()-start\n",
    "        #print(\"replay new time = %f\" % (start))\n",
    "        print(\"Game:\",gm+1,\"Scores:\",g.score1,g.score2,\"Aquired data:\",len(agent.memory))\n",
    "        score_plot.append(g.score2-g.score1)\n",
    "        counter_plot.append(gm)\n",
    "        del(g)\n",
    "        #input()\n",
    "    print(\"Finished training!\")\n",
    "    agent.model.save_weights('Mungala_weights.hdf5')\n",
    "    plot_seaborn(counter_plot, score_plot)\n",
    "    return 0\n",
    "#p2 = Player(\"DQN\",agent=Agent,depth=20)\n",
    "p1 = Player(\"human\",depth=5)\n",
    "p2 = Player(\"minmax\",depth=5)\n",
    "run(p1,p2,1000)\n",
    "#Agent = DQNAgent()\n",
    "#trainer = Player(\"minmax\",depth=5)\n",
    "#train(Agent,trainer,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4898/4898 [==============================] - 1s 287us/step - loss: 128.3188\n",
      "Epoch 2/10\n",
      "4898/4898 [==============================] - 1s 265us/step - loss: 128.3444\n",
      "Epoch 3/10\n",
      "4898/4898 [==============================] - 1s 255us/step - loss: 128.3083\n",
      "Epoch 4/10\n",
      "4898/4898 [==============================] - 1s 253us/step - loss: 128.2767\n",
      "Epoch 5/10\n",
      "4898/4898 [==============================] - 1s 250us/step - loss: 128.2850\n",
      "Epoch 6/10\n",
      "4898/4898 [==============================] - 1s 248us/step - loss: 128.2571\n",
      "Epoch 7/10\n",
      "4898/4898 [==============================] - 1s 244us/step - loss: 128.2163\n",
      "Epoch 8/10\n",
      "4898/4898 [==============================] - 1s 238us/step - loss: 128.1803\n",
      "Epoch 9/10\n",
      "4898/4898 [==============================] - 1s 230us/step - loss: 128.1555\n",
      "Epoch 10/10\n",
      "4898/4898 [==============================] - 1s 216us/step - loss: 128.1194 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x288a1c77b38>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=True\n",
    "for state, action, reward, next_state, done in Agent.memory:\n",
    "    target = reward\n",
    "    if not done:\n",
    "        target = reward + Agent.gamma * np.amax(Agent.model.predict(np.array([next_state]))[0])\n",
    "    target_f = Agent.model.predict(np.array([state]))\n",
    "    target_f[0][np.argmax(action)] = target\n",
    "    if p:\n",
    "        states=state\n",
    "        targets = target_f\n",
    "        p=False\n",
    "    else:\n",
    "        states=np.vstack((states, state))\n",
    "        targets=np.vstack((targets,target_f))\n",
    "#print(states.shape)\n",
    "model\n",
    "Agent.model.fit(states.reshape(len(states),10), targets.reshape(len(targets),6), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1598/1598 [==============================] - 2s 2ms/step - loss: 129.3975 - acc: 0.1471\n",
      "Epoch 2/100\n",
      "1598/1598 [==============================] - 0s 296us/step - loss: 129.3588 - acc: 0.1865\n",
      "Epoch 3/100\n",
      "1598/1598 [==============================] - 0s 268us/step - loss: 129.3787 - acc: 0.1915\n",
      "Epoch 4/100\n",
      "1598/1598 [==============================] - 0s 270us/step - loss: 129.3589 - acc: 0.2234\n",
      "Epoch 5/100\n",
      "1598/1598 [==============================] - 0s 295us/step - loss: 129.3625 - acc: 0.2616\n",
      "Epoch 6/100\n",
      "1598/1598 [==============================] - 0s 299us/step - loss: 129.2913 - acc: 0.2866\n",
      "Epoch 7/100\n",
      "1598/1598 [==============================] - 0s 300us/step - loss: 129.2551 - acc: 0.3204\n",
      "Epoch 8/100\n",
      "1598/1598 [==============================] - 0s 309us/step - loss: 129.2880 - acc: 0.3548\n",
      "Epoch 9/100\n",
      "1598/1598 [==============================] - 0s 295us/step - loss: 129.3006 - acc: 0.3892\n",
      "Epoch 10/100\n",
      "1598/1598 [==============================] - 0s 294us/step - loss: 129.3079 - acc: 0.3999\n",
      "Epoch 11/100\n",
      "1598/1598 [==============================] - 0s 281us/step - loss: 129.3439 - acc: 0.4312\n",
      "Epoch 12/100\n",
      "1598/1598 [==============================] - 0s 292us/step - loss: 129.2402 - acc: 0.4681\n",
      "Epoch 13/100\n",
      "1598/1598 [==============================] - 0s 298us/step - loss: 129.2288 - acc: 0.4743\n",
      "Epoch 14/100\n",
      "1598/1598 [==============================] - 0s 300us/step - loss: 129.2427 - acc: 0.4931\n",
      "Epoch 15/100\n",
      "1598/1598 [==============================] - 0s 285us/step - loss: 129.2072 - acc: 0.5219\n",
      "Epoch 16/100\n",
      "1598/1598 [==============================] - 0s 291us/step - loss: 129.2726 - acc: 0.5138\n",
      "Epoch 17/100\n",
      "1598/1598 [==============================] - 0s 307us/step - loss: 129.2286 - acc: 0.5288\n",
      "Epoch 18/100\n",
      "1598/1598 [==============================] - 0s 296us/step - loss: 129.2538 - acc: 0.5219\n",
      "Epoch 19/100\n",
      "1598/1598 [==============================] - 0s 292us/step - loss: 129.2546 - acc: 0.5526\n",
      "Epoch 20/100\n",
      " 544/1598 [=========>....................] - ETA: 0s - loss: 119.7119 - acc: 0.5772"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-eb977b0e6541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='relu', input_dim=10))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "model.add(Dense(units=6, activation='softmax'))\n",
    "opt = Adam(Agent.learning_rate)\n",
    "model.compile(loss='mse', optimizer=opt,metrics=['accuracy'])\n",
    "model.fit(states.reshape(len(states),10), targets.reshape(len(targets),6), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "\n",
    "\n",
    "class DQNAgent(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reward = 0\n",
    "        self.gamma = 0.9\n",
    "        self.dataframe = pd.DataFrame()\n",
    "        self.short_memory = np.array([])\n",
    "        self.agent_target = 1\n",
    "        self.agent_predict = 0\n",
    "        self.learning_rate = 0.0005\n",
    "        self.model = self.network()\n",
    "        #self.model = self.network(\"weights.hdf5\")\n",
    "        self.epsilon = 0\n",
    "        self.actual = []\n",
    "        self.memory = []\n",
    "    def get_state(self, game):\n",
    "        return np.asarray(game.board)\n",
    "    def set_reward(self, old_delta, new_delta):\n",
    "        self.reward = 0\n",
    "        self.reward = (new_delta - old_delta) * 10\n",
    "        return self.reward\n",
    "\n",
    "    def network(self, weights=None):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=120, activation='relu', input_dim=10))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(Dense(units=120, activation='relu'))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(Dense(units=120, activation='relu'))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(Dense(units=6, activation='softmax'))\n",
    "        opt = Adam(self.learning_rate)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        if weights:\n",
    "            model.load_weights(weights)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state,done):\n",
    "        self.memory.append((state, action, reward, next_state,done))\n",
    "\n",
    "    def replay_new(self, memory):\n",
    "        if len(memory) > 1000:\n",
    "            minibatch = random.sample(memory, 1000)\n",
    "        else:\n",
    "            minibatch = memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n",
    "            target_f = self.model.predict(np.array([state]))\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
    "        \n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + self.gamma * np.amax(self.model.predict(next_state.reshape((1, 10)))[0])\n",
    "        target_f = self.model.predict(state.reshape((1, 10)))\n",
    "        target_f[0][np.argmax(action)] = target\n",
    "        self.model.fit(state.reshape((1, 10)), target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
